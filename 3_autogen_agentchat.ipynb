{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6bc387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84519d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets have a single one\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9bc475",
   "metadata": {},
   "source": [
    "### Will define our agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f503f",
   "metadata": {},
   "source": [
    "##### every agent will have an AgentID which will have 2 componenst\n",
    "agent.id.type descibe the tyoe of agent\n",
    "agent.id.key gives its unique indetifier\n",
    "Any method with the @message_handler decorated will have the opportunity to recive the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"Simple\")\n",
    "        \n",
    "        @message_handler\n",
    "        async def on_my_messsage(self, message: Message, ctx: MessageContext) -> Message:\n",
    "            return Message(content=f\"This is (self.id.type)-(self.id.key). You said '{message.content}' and I disagree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab298f1d",
   "metadata": {},
   "source": [
    "#### will create a standalone runtime and regsiter our agent type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b23e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"Simple\")\n",
    "    \n",
    "    @message_handler\n",
    "    async def on_my_messsage(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        return Message(content=f\"This is {self.id.type}-{self.id.key}. You said '{message.content}' and I disagree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d09af",
   "metadata": {},
   "source": [
    "### Lets start the runtime and send message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81074099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentType(type='simple_agent')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "await SimpleAgent.register(runtime, \"simple_agent\", lambda: SimpleAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ec1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97155d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> This is simple_agent-default. You said 'Well hi there!' and I disagree.\n"
     ]
    }
   ],
   "source": [
    "agent_id = AgentId(\"simple_agent\", \"default\")\n",
    "response = await runtime.send_message(Message(\"Well hi there!\"), agent_id)\n",
    "print(\">>>\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acdad818",
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede0b4f",
   "metadata": {},
   "source": [
    "#### Will use agentchat assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "322a757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLLMAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"LLMAgent\")\n",
    "        model_clinet = OllamaChatCompletionClient(model=\"mistral\")\n",
    "        self._delegate = AssistantAgent(\"LLMAgent\", model_client=model_clinet)\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        print(f\"{self.id.type} received message: {message.content}\")\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.handle_message([text_message], ctx.cancellation_token)\n",
    "        reply = response.chat_message.content\n",
    "        print(f\"{self.id.type} respond: {reply}\")\n",
    "        return Message(content=reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97b70a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentType(type='LLMAgent')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "\n",
    "runtime = SingleThreadedAgentRuntime()\n",
    "await SimpleAgent.register(runtime, \"simple_agent\", lambda: SimpleAgent())\n",
    "await MyLLMAgent.register(runtime, \"LLMAgent\", lambda: MyLLMAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb5b665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMAgent received message: Hello, there!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AssistantAgent' object has no attribute 'handle_message'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m runtime.start()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m runtime.send_message(Message(\u001b[33m\"\u001b[39m\u001b[33mHello, there!\u001b[39m\u001b[33m\"\u001b[39m), AgentId(\u001b[33m\"\u001b[39m\u001b[33mLLMAgent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m>>>\u001b[39m\u001b[33m\"\u001b[39m, response.content)\n\u001b[32m      4\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m runtime.send_message(Message(response.content), AgentId(\u001b[33m\"\u001b[39m\u001b[33msimple_agent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic AI Course/AutoGen/venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:384\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime.send_message\u001b[39m\u001b[34m(self, message, recipient, sender, cancellation_token, message_id)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._message_queue.put(\n\u001b[32m    371\u001b[39m     SendMessageEnvelope(\n\u001b[32m    372\u001b[39m         message=message,\n\u001b[32m   (...)\u001b[39m\u001b[32m    379\u001b[39m     )\n\u001b[32m    380\u001b[39m )\n\u001b[32m    382\u001b[39m cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic AI Course/AutoGen/venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:507\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime._process_send\u001b[39m\u001b[34m(self, message_envelope)\u001b[39m\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tracer_helper.trace_block(\n\u001b[32m    496\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprocess\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    497\u001b[39m         recipient_agent.id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    504\u001b[39m         ),\n\u001b[32m    505\u001b[39m     ):\n\u001b[32m    506\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m MessageHandlerContext.populate_context(recipient_agent.id):\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m             response = \u001b[38;5;28;01mawait\u001b[39;00m recipient_agent.on_message(\n\u001b[32m    508\u001b[39m                 message_envelope.message,\n\u001b[32m    509\u001b[39m                 ctx=message_context,\n\u001b[32m    510\u001b[39m             )\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CancelledError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message_envelope.future.cancelled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic AI Course/AutoGen/venv/lib/python3.12/site-packages/autogen_core/_base_agent.py:119\u001b[39m, in \u001b[36mBaseAgent.on_message\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: Any, ctx: MessageContext) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_message_impl(message, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic AI Course/AutoGen/venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py:485\u001b[39m, in \u001b[36mRoutedAgent.on_message_impl\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    484\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m h.router(message, ctx):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m h(\u001b[38;5;28mself\u001b[39m, message, ctx)\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_unhandled_message(message, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic AI Course/AutoGen/venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py:149\u001b[39m, in \u001b[36mmessage_handler.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    147\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in target types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m return_value = \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, message, ctx)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m AnyType \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m return_types \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(return_value) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m return_types:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mMyLLMAgent.handle_my_message\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.id.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m received message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m text_message = TextMessage(content=message.content, source=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_delegate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_message\u001b[49m([text_message], ctx.cancellation_token)\n\u001b[32m     12\u001b[39m reply = response.chat_message.content\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.id.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m respond: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreply\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'AssistantAgent' object has no attribute 'handle_message'"
     ]
    }
   ],
   "source": [
    "runtime.start()\n",
    "response = await runtime.send_message(Message(\"Hello, there!\"), AgentId(\"LLMAgent\", \"default\"))\n",
    "print(\">>>\", response.content)\n",
    "response = await runtime.send_message(Message(response.content), AgentId(\"simple_agent\", \"default\"))\n",
    "print(\">>>\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdc6fd",
   "metadata": {},
   "source": [
    "## Will make a three agent interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f338005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OllamaChatCompletionClient(model=\"llama3.2:1b\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> \"Message\":\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], cancellation_token=ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OllamaChatCompletionClient(model=\"llama3.2:1b\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> \"Message\":\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], cancellation_token=ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89c5f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE = \"You are judging a game of rock, paper, scissors. The players have made these choices:\\n\"\n",
    "\n",
    "class RockPaperScissorsAgent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OllamaChatCompletionClient(model=\"llama3.2:1b\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        instruction = \"You are playing rock, paper, scissors. Respond with either 'rock', 'paper', or 'scissors'.\"\n",
    "        message = Message(content=instruction)\n",
    "        inner_1 = AgentId(\"Player1\", \"default\")\n",
    "        inner_2 = AgentId(\"Player2\", \"default\")\n",
    "        response1 = await self.send_message(message, inner_1)\n",
    "        response2 = await self.send_message(message, inner_2)\n",
    "        result = f\"Player 1: {response1.content}\\nPlayer 2: {response2.content}.\\n\"\n",
    "        judgement = f\"{JUDGE}{result} Who wins?\"\n",
    "        \n",
    "        text_message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], cancellation_token=ctx.cancellation_token)\n",
    "        return Message(content = result + response.chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35b810ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have to register\n",
    "runtime = SingleThreadedAgentRuntime()\n",
    "await Player1Agent.register(runtime, \"Player1\", lambda: Player1Agent(\"Player1\"))\n",
    "await Player2Agent.register(runtime, \"Player2\", lambda: Player2Agent(\"Player2\"))\n",
    "await RockPaperScissorsAgent.register(runtime, \"rock_paper_scissors\", lambda: RockPaperScissorsAgent(\"rock_paper_scissors\"))\n",
    "runtime.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52aadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1: I have my choice.\n",
      "Player 2: I have responded. My turn: rock..\n",
      "Since Player 1 has chosen \"rock,\" and Player 2 has chosen \"paper\", according to the rules of rock, paper, scissors, Paper covers Rock, so Player 2 wins this round.\n",
      "\n",
      "The score is now:\n",
      "Player 1: 0\n",
      "Player 2: 1\n",
      "\n",
      "Would you like me to continue with the next round?\n"
     ]
    }
   ],
   "source": [
    "agent_id = AgentId(\"rock_paper_scissors\", \"default\")\n",
    "message = Message(content=\"go\")\n",
    "response = await runtime.send_message(message, agent_id)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7563da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc7c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae34f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8306b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
